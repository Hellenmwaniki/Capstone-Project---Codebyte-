{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249e1a0a",
   "metadata": {},
   "source": [
    "## Capstone Project Title: Developmet of a Natural Language Multi-lingual Chatbot for the Kenyan Market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fa9c5",
   "metadata": {},
   "source": [
    "## Team Members \n",
    "\n",
    "1. Jessica Mutiso\n",
    "2. Brian Waweru\n",
    "3. Pamela Godia\n",
    "4. Hellen Mwaniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45b30b",
   "metadata": {},
   "source": [
    "## 1. Project Overview \n",
    "\n",
    "This project aims to develop a natural language chatbot capable of generating human-like responses and understanding informal customer feedback expressed in English, Kenyan Swahili and Sheng. Designed for a startup expanding into the Kenyan market, the chatbot will help the company engage users more naturally and analyze feedback from social platforms and online conversations. By training on locally relevant dialogue data  including YouTube comments and Kenyan media the system will capture the linguistic and cultural nuances often missed by standard models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95e599",
   "metadata": {},
   "source": [
    "# 1.1. Stakeholders\n",
    "\n",
    "Stakeholders in the Technonlogy Industry wanting to have a start-up in Kenya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae1080",
   "metadata": {},
   "source": [
    "## 1.2 Problem Statement\n",
    "Startups entering new markets often struggle to understand customer feedback when it's expressed in local dialects or informal language. In Kenya, much of this communication occurs in Swahili and Sheng, which combine local slang, English, and Swahili in a fluid, often unstructured manner. Existing chatbot systems trained on formal English fail to grasp the tone, intent, or meaning behind such messages. This project aims to fill that gap by building a chatbot trained specifically on real-world Kenyan conversations to interpret and respond to customer queries and feedback with local context and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004a63c",
   "metadata": {},
   "source": [
    "## 1.3 Objectives\n",
    "\n",
    "- Collect and preprocess Kenyan user dialogue from YouTube, social media, and local content featuring Swahili and Sheng\n",
    "\n",
    "- Fine-tune the chatbot with foundational data for conversational structure, while emphasizing local language patterns\n",
    "\n",
    "- Build a sequence-to-sequence model  capable of handling informal, code-switched dialogue\n",
    "\n",
    "- Evaluate the chatbot‚Äôs performance with emphasis on contextual relevance and local understanding\n",
    "\n",
    "- Present a working prototype that simulates real customer feedback scenarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94715249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, csv\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b28f3f",
   "metadata": {},
   "source": [
    "# 2.0 Data Scraping and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6687ec1",
   "metadata": {},
   "source": [
    "The Cornell Movie Dialogues Corpus contains fictional conversations from movie scripts. It is made up of several text files, but the two we are using here are:\n",
    "\n",
    "- `movie_lines.txt` ‚Äì contains individual lines of dialogue.\n",
    "\n",
    "- `movie_conversations.txt` ‚Äì contains sequences of line IDs, showing how those lines form a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd197d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation 1\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "Conversation 2\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "Conversation 3\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "Seems like she could get a date easy enough...\n"
     ]
    }
   ],
   "source": [
    "# load the cornell `movie_lines.txt` dataset\n",
    "# step 1 - Loading and Parsing movie_lines.txt\n",
    "# GOAL: Create a dict `id2line` that maps a `line ID` to its actual dialogue.\n",
    "movie_lines_path = r'original-data\\movie_lines.txt'\n",
    "# the dictionary\n",
    "id2line = {}\n",
    "# parsing\n",
    "with open(movie_lines_path, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 5:\n",
    "            line_id, _, _, _, text = parts\n",
    "            id2line[line_id] = text\n",
    "# step 2 - Load and Parse `movie_conversations.txt`\n",
    "# GOAL: To create a list of conversations, where each conversation \n",
    "# is a list of line IDs i.e. Extract conversations - list of line IDs\n",
    "movie_conversations_path = r\"original-data\\movie_conversations.txt\"\n",
    "# list of conversations\n",
    "conversations = []\n",
    "# parsing\n",
    "with open(movie_conversations_path, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 4:\n",
    "            line_ids_str = parts[3]\n",
    "            # Convert string to actual list of line IDs\n",
    "            line_ids = eval(line_ids_str)  # Safe here because the format is consistent\n",
    "            conversations.append(line_ids)\n",
    "# step 3 - Reconstruct Conversations from Line IDs\n",
    "# GOAL: Convert line IDs back into actual text using the \n",
    "# `id2line` dictionary.\n",
    "# dictionary container variable\n",
    "reconstructed_conversations = []\n",
    "#  parsing\n",
    "for conv in conversations:\n",
    "    dialogue = []\n",
    "    for line_id in conv:\n",
    "        line_text = id2line.get(line_id, \"\")\n",
    "        dialogue.append(line_text)\n",
    "    reconstructed_conversations.append(dialogue)\n",
    "# step 4 - View Sample Conversations\n",
    "# first 3 conversations\n",
    "for i, convo in enumerate(reconstructed_conversations[:3]):\n",
    "    print(f\"\\nConversation {i+1}\")\n",
    "    for turn in convo:\n",
    "        print(turn)\n",
    "# Save to Text File\n",
    "with open(\"reconstructed_conversations.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for convo in reconstructed_conversations:\n",
    "        for turn in convo:\n",
    "            f.write(turn + \"\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7052fd",
   "metadata": {},
   "source": [
    "### 2.1 Saving the `reconstructed_conversations` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e901df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Yes, file successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Target folder \n",
    "output_folder = \"chatbot-repo\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Full path to save the file\n",
    "output_file_path = os.path.join(output_folder, \"reconstructed_conversations.txt\")\n",
    "\n",
    "# Save the reconstructed conversations into the file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for convo in reconstructed_conversations:\n",
    "        for turn in convo:\n",
    "            f.write(turn + \"\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# confirmation\n",
    "## print(f\"File saved to: {os.path.abspath(output_file_path)}\")\n",
    "# Confirm that the file exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(\"‚úÖ Yes, file successfully saved!\")\n",
    "    # print(\"üìÅ Location:\", os.path.abspath(output_file_path))\n",
    "else:\n",
    "    print(\"‚ùå Failed to save the file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a526e1a",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- `id2line` has mapped line IDs to actual dialogue text.\n",
    "\n",
    "- `conversations` then holds the sequences of line IDs per conversation.\n",
    "\n",
    "- `reconstructed_conversations` gives you the full text of the conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85745774",
   "metadata": {},
   "source": [
    "### 2.2 YouTube datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb047800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamela's file has the shape: (25629, 2)\n",
      "Brian's file has the shape: (2853, 2)\n",
      "\n",
      "\n",
      "Pamela's YouTube File\n",
      "      video_id                                            comment\n",
      "0  qlZM3McwO1Q  What an incredible victory. I agree the Kenyan...\n",
      "1  qlZM3McwO1Q                                                  ‚ù§\n",
      "2  qlZM3McwO1Q  ‚ÄúClaudia is an amazonian goddess with a beauti...\n",
      "3  qlZM3McwO1Q  Proud of my motherland Kenya ‚ù§‚ù§‚ù§and Africa.at ...\n",
      "4  qlZM3McwO1Q                                               Damn\n",
      "\n",
      "\n",
      "Brian's YouTube File\n",
      "                                         Top Comment Reply\n",
      "0  Apple missed the boat on AI OR... Apple is doi...   NaN\n",
      "1  Who added the background music to the video it...   NaN\n",
      "2                    16:26  FEMI KUTI !!! RAAHHH !!!   NaN\n",
      "3            The greatest AI scam in history, is AI.   NaN\n",
      "4  if I only knew Siri is a mess, I would bought ...   NaN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yt_file_path_pamela = r'original-data\\pamela-youtube_comments.csv'\n",
    "yt_file_path_brian = r'original-data\\brian_youtube_data_comments.csv'\n",
    "\n",
    "# Pamela's file\n",
    "yt_1 = pd.read_csv(yt_file_path_pamela)\n",
    "print(\"Pamela's file has the shape:\", yt_1.shape)\n",
    "\n",
    "# Brian's file\n",
    "yt_2 = pd.read_csv(yt_file_path_brian)\n",
    "print(\"Brian's file has the shape:\", yt_2.shape)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Pamela's YouTube File\")\n",
    "print(yt_1.head(), end = '\\n\\n\\n')\n",
    "\n",
    "print(\"Brian's YouTube File\")\n",
    "print(yt_2.head(), end = '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc72b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamela's YouTube File\n",
      "Now, Pamela's file has the shape: (25629, 1)\n",
      "                                             comment\n",
      "0  What an incredible victory. I agree the Kenyan...\n",
      "1                                                  ‚ù§\n",
      "2  ‚ÄúClaudia is an amazonian goddess with a beauti...\n",
      "3  Proud of my motherland Kenya ‚ù§‚ù§‚ù§and Africa.at ...\n",
      "4                                               Damn\n",
      "\n",
      "\n",
      "Brian's YouTube File\n",
      "Now, Brian's file has the shape: (2853, 1)\n",
      "                                         Top Comment\n",
      "0  Apple missed the boat on AI OR... Apple is doi...\n",
      "1  Who added the background music to the video it...\n",
      "2                    16:26  FEMI KUTI !!! RAAHHH !!!\n",
      "3            The greatest AI scam in history, is AI.\n",
      "4  if I only knew Siri is a mess, I would bought ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning yt_1 file\n",
    "## i.e dropping first column\n",
    "yt_1 = yt_1.drop(yt_1.columns[0], axis=1)\n",
    "\n",
    "# Cleaning yt_2 file\n",
    "yt_2 = yt_2.drop(yt_2.columns[1], axis=1)\n",
    "\n",
    "print(\"Pamela's YouTube File\")\n",
    "print(\"Now, Pamela's file has the shape:\", yt_1.shape)\n",
    "print(yt_1.head(), end = '\\n\\n\\n')\n",
    "\n",
    "print(\"Brian's YouTube File\")\n",
    "print(\"Now, Brian's file has the shape:\", yt_2.shape)\n",
    "print(yt_2.head(), end = '\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eccb3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of the combine YouTube data is: (28482, 2)\n"
     ]
    }
   ],
   "source": [
    "# combineing the two together\n",
    "yt_combined = pd.concat([yt_1, yt_2], axis=0, ignore_index=True)\n",
    "# Thus...\n",
    "print(\"The Shape of the combine YouTube data is:\", yt_combined.shape)\n",
    "\n",
    "# comfirming succesful stacking\n",
    "# print(yt_1.shape[0] + yt_2.shape[0] == yt_combined.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e61d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, `yt_combined` has been saved to chatbot-repo.\n"
     ]
    }
   ],
   "source": [
    "# Folder and file path parameters\n",
    "folder_path = 'chatbot-repo'\n",
    "file_path = os.path.join(folder_path, 'yt_combined.txt')\n",
    "\n",
    "# Save the DataFrame to a txt file\n",
    "yt_combined.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Success, `yt_combined` has been saved to {folder_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4145f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined and saved to chatbot-repo\\nlp_data.txt\n"
     ]
    }
   ],
   "source": [
    "# File names\n",
    "file1 = r'chatbot-repo\\reconstructed_conversations.txt' # Cornell-uni\n",
    "file2 = r'chatbot-repo\\yt_combined.txt' # youtube-comments\n",
    "nlp_data = r'chatbot-repo\\nlp_data.txt'  # Output file is literally named \"nlp_data.txt\"\n",
    "\n",
    "# Read both files\n",
    "with open(file1, \"r\", encoding=\"utf-8\") as f1, open(file2, \"r\", encoding=\"utf-8\") as f2:\n",
    "    content1 = f1.read()\n",
    "    content2 = f2.read()\n",
    "\n",
    "# Combine contents\n",
    "combined_data = content1 + \"\\n\" + content2\n",
    "\n",
    "# Write to output file\n",
    "with open(nlp_data, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(combined_data)\n",
    "\n",
    "print(f\"Files combined and saved to {nlp_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de6589",
   "metadata": {},
   "source": [
    "# 3.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58336f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# Function to clean the data\n",
    "def clean_text(text):\n",
    "\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()                          # Lowercase all text\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)             # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)             # Remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)          # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)              # Remove digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()     # Remove extra whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ada4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "def clean_text_file(input_file, output_file=\"cleaned_output.txt\"):\n",
    "    # Read the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        original_text = f.read()\n",
    "    \n",
    "    # Clean the text\n",
    "    if pd.isnull(original_text):\n",
    "        cleaned_text = \"\"\n",
    "    else:\n",
    "        cleaned_text = original_text.lower()\n",
    "        cleaned_text = re.sub(r\"http\\S+|www\\S+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"@\\w+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"#\\w+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "\n",
    "    # Write the cleaned text to a new file in the same directory\n",
    "    input_dir = os.path.dirname(input_file)\n",
    "    output_path = os.path.join(input_dir, output_file)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "    print(f\"Cleaned text written to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c83622",
   "metadata": {},
   "source": [
    "# 4.0 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6eb9655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text written to: chatbot-repo\\cleaned_output.txt\n"
     ]
    }
   ],
   "source": [
    "# renaming the cleaned nlp_data_file to chatbot-repo\n",
    "\n",
    "nlp_data_file = r'chatbot-repo\\nlp_data.txt'\n",
    "\n",
    "clean_text_file(nlp_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3c7b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chatbot-repo\\\\nlp_data.txt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2987b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot-repo\\nlp_data.txt\n"
     ]
    }
   ],
   "source": [
    "print(nlp_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e43c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "----------------------------------------\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "----------------------------------------\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "Seems like she could get a date easy enough...\n",
      "\n",
      "----------------------------------------\n",
      "Why?\n",
      "Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "That's a shame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing the data \n",
    "file_path = 'chatbot-repo\\\\nlp_data.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i in range(20):  # Change 20 to however many lines you want to preview\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd4f62",
   "metadata": {},
   "source": [
    "## 5.0 Baseline Model - TF-IDF + Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a2912",
   "metadata": {},
   "source": [
    "Converting file into prompt-response pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700bbccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Response: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Prompt: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Response: Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Prompt: Not the hacking and gagging and spitting part.  Please.\n",
      "Response: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "Prompt: You're asking me out.  That's so cute. What's your name again?\n",
      "Response: Forget it.\n",
      "\n",
      "Prompt: No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Response: Cameron.\n",
      "\n",
      "Prompt: Cameron.\n",
      "Response: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "\n",
      "Prompt: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "Response: Seems like she could get a date easy enough...\n",
      "\n",
      "Prompt: Why?\n",
      "Response: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "\n",
      "Prompt: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "Response: That's a shame.\n",
      "\n",
      "Prompt: Gosh, if only we could find Kat a boyfriend...\n",
      "Response: Let me see what I can do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'chatbot-repo\\\\nlp_data.txt'\n",
    "\n",
    "# Load and split into blocks\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content by scene/block separator\n",
    "blocks = content.split('----------------------------------------')\n",
    "\n",
    "prompt_response_pairs = []\n",
    "\n",
    "for block in blocks:\n",
    "    # Clean and split into lines\n",
    "    lines = [line.strip() for line in block.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    # Pair consecutive lines as prompt-response\n",
    "    for i in range(len(lines) - 1):\n",
    "        prompt = lines[i]\n",
    "        response = lines[i + 1]\n",
    "        prompt_response_pairs.append((prompt, response))\n",
    "\n",
    "# Preview a few pairs\n",
    "for i, (prompt, response) in enumerate(prompt_response_pairs[:10]):\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e21036",
   "metadata": {},
   "source": [
    "Vectorizing all prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77db608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming prompt_response_pairs has already been extracted\n",
    "prompts = [pair[0] for pair in prompt_response_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef05a5d",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b5c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix shape: (254094, 58575)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the prompts\n",
    "tfidf_matrix = vectorizer.fit_transform(prompts)\n",
    "\n",
    "# Output shape: (n_prompts, n_features)\n",
    "print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f0b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254094, 58575)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affe2c7",
   "metadata": {},
   "source": [
    "Compute Cosine Similarity between the user's input and your stored prompts, then return the best-matching response using TF-IDF + Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b11fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_best_response(user_input, vectorizer, tfidf_matrix, prompt_response_pairs, threshold=0.2):\n",
    "    \n",
    "    # Vectorize the user input\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity with all prompts\n",
    "    similarity_scores = cosine_similarity(user_vec, tfidf_matrix)\n",
    "\n",
    "    # Get the index of the best-matching prompt\n",
    "    best_index = similarity_scores.argmax()\n",
    "    best_score = similarity_scores[0, best_index]\n",
    "\n",
    "    if best_score < threshold:\n",
    "        return \"Sorry, I don't understand.\"\n",
    "\n",
    "    # Return the corresponding response\n",
    "    return prompt_response_pairs[best_index][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c389d45",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c74bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I don't know...  One of us has to be here in case Arbogast's on the way.\n"
     ]
    }
   ],
   "source": [
    "# Assume prompt_response_pairs = [(prompt1, response1), (prompt2, response2), ...]\n",
    "user_input = \"Why can't she go out with someone?\"\n",
    "response = get_best_response(user_input, vectorizer, tfidf_matrix, prompt_response_pairs)\n",
    "\n",
    "print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bcb5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Berger, Norwegian, and at your service, sir.\n"
     ]
    }
   ],
   "source": [
    "# Assume prompt_response_pairs = [(prompt1, response1), (prompt2, response2), ...]\n",
    "user_input = \"What is your name?\"\n",
    "response = get_best_response(user_input, vectorizer, tfidf_matrix, prompt_response_pairs)\n",
    "\n",
    "print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68bcb755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Paul, can you hand me the olives? Ruth, I need you to, what was it?\n"
     ]
    }
   ],
   "source": [
    "# Assume prompt_response_pairs = [(prompt1, response1), (prompt2, response2), ...]\n",
    "user_input = \"How can i be of help today?\"\n",
    "response = get_best_response(user_input, vectorizer, tfidf_matrix, prompt_response_pairs)\n",
    "\n",
    "print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cbace",
   "metadata": {},
   "source": [
    "Optional Optimization (for very large datasets)\n",
    "\n",
    "*Use sklearn.metrics.pairwise.linear_kernel instead of cosine_similarity for faster dot-product similarity (same result for normalized vectors like TF-IDF).\n",
    "\n",
    "*Index the TF-IDF matrix with FAISS or Annoy for approximate nearest neighbors if performance becomes an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448a8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "def get_best_response_fast(user_input, vectorizer, tfidf_matrix, prompt_response_pairs, threshold=0.2):\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "    cosine_similarities = linear_kernel(user_vec, tfidf_matrix).flatten()\n",
    "\n",
    "    best_index = cosine_similarities.argmax()\n",
    "    best_score = cosine_similarities[best_index]\n",
    "\n",
    "    if best_score < threshold:\n",
    "        return \"Sorry, I don't understand.\"\n",
    "\n",
    "    return prompt_response_pairs[best_index][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80538c",
   "metadata": {},
   "source": [
    "Tips:\n",
    "threshold=0.2 avoids returning weak matches; tune it based on your dataset.\n",
    "\n",
    "Strip punctuation/lowercase your prompts and user input for better matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01f7e1",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df56c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ef646",
   "metadata": {},
   "source": [
    "Save the TF-IDF vectorizer, matrix, and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2f0f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prompt_response_pairs.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save TF-IDF matrix (can be large, but compressed)\n",
    "joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')\n",
    "\n",
    "# Save prompt-response pairs (list of tuples)\n",
    "joblib.dump(prompt_response_pairs, 'prompt_response_pairs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a2af8",
   "metadata": {},
   "source": [
    "Organize the files in a folder called model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
